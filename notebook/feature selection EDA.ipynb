{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import src.util as util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config_data: dict) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    # Load every set of data\n",
    "    clean_data = util.pickle_load(config_data['clean_dataset_path'])\n",
    "\n",
    "    x_train = util.pickle_load(config_data[\"train_set_path\"][0])\n",
    "    y_train = util.pickle_load(config_data[\"train_set_path\"][1])\n",
    "\n",
    "    x_valid = util.pickle_load(config_data[\"valid_set_path\"][0])\n",
    "    y_valid = util.pickle_load(config_data[\"valid_set_path\"][1])\n",
    "\n",
    "    x_test = util.pickle_load(config_data[\"test_set_path\"][0])\n",
    "    y_test = util.pickle_load(config_data[\"test_set_path\"][1])\n",
    "\n",
    "    # Concatenate x and y each set\n",
    "    train_set = pd.concat([x_train, y_train], axis = 1)\n",
    "    valid_set = pd.concat([x_valid, y_valid], axis = 1)\n",
    "    test_set = pd.concat([x_test, y_test], axis = 1)\n",
    "\n",
    "    # Return 3 set of data\n",
    "    return clean_data, train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data, train_set, valid_set, test_set = load_dataset(config_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stock Return Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the way to normalize all of data value, its relevant if we change them into return percentage.\n",
    "# the advantage are: \n",
    "# 1. the data value will vary from -0.5 to +0.5. While its possible, its less likely stock change will be up/down more than 50% within 2 days. \n",
    "# 2. the stock return is something we want to know anyway therefore its a representative approach in this case\n",
    "\n",
    "def transform_to_stock_return(dataset, params):\n",
    "    # define the return for all stock based on the next day of its price change percentage \n",
    "    dataset = (dataset.shift(periods=1)-dataset)*100/dataset\n",
    "    \n",
    "    #define the target return column name\n",
    "    target_return_column_name = f\"{params['target']} Return D+2\"\n",
    "    \n",
    "    # add additional column of our targeted stock return\n",
    "    dataset[target_return_column_name] = dataset[params['target']].shift(periods=-2)\n",
    "\n",
    "    # handling missing value of shifted targeted column & its reference column\n",
    "    dataset.dropna(subset=params['target'], inplace=True)\n",
    "    dataset.dropna(subset=target_return_column_name, inplace=True)\n",
    "\n",
    "    # handling missing value of the remaining columns\n",
    "    #dataset.fillna(0, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,n_std):\n",
    "    for col in df.columns:\n",
    "        #print('Working on column: {}'.format(col))\n",
    "        \n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean+(n_std*sd))]\n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_feng = transform_to_stock_return(dataset=train_set, params=config_data)\n",
    "train_set_feng = remove_outliers(train_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_feng = transform_to_stock_return(dataset=valid_set, params=config_data)\n",
    "val_set_feng = remove_outliers(val_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_feng = transform_to_stock_return(dataset=test_set, params=config_data)\n",
    "test_set_feng = remove_outliers(test_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-09-11 03:00:00</td>\n",
       "      <td>-4.051934</td>\n",
       "      <td>2.385383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2022-07-05 00:00:00</td>\n",
       "      <td>-25.925926</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-08-02 00:00:00</td>\n",
       "      <td>-7.204922</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-09-12 12:00:00</td>\n",
       "      <td>-3.253968</td>\n",
       "      <td>3.738513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-10-10 06:00:00</td>\n",
       "      <td>-0.161988</td>\n",
       "      <td>6.952519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022-11-25 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.071942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.062522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date        Min        Max\n",
       "count                   24  24.000000  24.000000\n",
       "mean   2022-09-11 03:00:00  -4.051934   2.385383\n",
       "min    2022-07-05 00:00:00 -25.925926   0.000000\n",
       "25%    2022-08-02 00:00:00  -7.204922   0.000000\n",
       "50%    2022-09-12 12:00:00  -3.253968   3.738513\n",
       "75%    2022-10-10 06:00:00  -0.161988   6.952519\n",
       "max    2022-11-25 00:00:00   0.000000  10.071942\n",
       "std                    NaN   0.000000   9.062522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(24, 760)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=train_set_feng))\n",
    "display(train_set_feng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-01-25 04:48:00</td>\n",
       "      <td>-3.703387</td>\n",
       "      <td>6.892291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-12 00:00:00</td>\n",
       "      <td>-25.700447</td>\n",
       "      <td>4.938272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-01-21 00:00:00</td>\n",
       "      <td>-13.162202</td>\n",
       "      <td>7.168676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-01-25 12:00:00</td>\n",
       "      <td>-4.083333</td>\n",
       "      <td>7.325424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-01-29 06:00:00</td>\n",
       "      <td>-1.992017</td>\n",
       "      <td>9.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-02-06 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.046288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date        Min        Max\n",
       "count                   10  10.000000  10.000000\n",
       "mean   2023-01-25 04:48:00  -3.703387   6.892291\n",
       "min    2023-01-12 00:00:00 -25.700447   4.938272\n",
       "25%    2023-01-21 00:00:00 -13.162202   7.168676\n",
       "50%    2023-01-25 12:00:00  -4.083333   7.325424\n",
       "75%    2023-01-29 06:00:00  -1.992017   9.318182\n",
       "max    2023-02-06 00:00:00   0.000000  11.111111\n",
       "std                    NaN   0.000000  13.046288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 760)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(val_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=val_set_feng))\n",
    "display(val_set_feng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-03-06 16:48:00</td>\n",
       "      <td>-4.481199</td>\n",
       "      <td>7.346397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-02-15 00:00:00</td>\n",
       "      <td>-25.438596</td>\n",
       "      <td>7.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-02-21 18:00:00</td>\n",
       "      <td>-11.341463</td>\n",
       "      <td>7.171001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-03-04 12:00:00</td>\n",
       "      <td>-3.383872</td>\n",
       "      <td>7.445716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-03-22 12:00:00</td>\n",
       "      <td>-1.860587</td>\n",
       "      <td>7.469120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-03-30 00:00:00</td>\n",
       "      <td>-0.617284</td>\n",
       "      <td>7.526882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.505263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date        Min        Max\n",
       "count                   10  10.000000  10.000000\n",
       "mean   2023-03-06 16:48:00  -4.481199   7.346397\n",
       "min    2023-02-15 00:00:00 -25.438596   7.051282\n",
       "25%    2023-02-21 18:00:00 -11.341463   7.171001\n",
       "50%    2023-03-04 12:00:00  -3.383872   7.445716\n",
       "75%    2023-03-22 12:00:00  -1.860587   7.469120\n",
       "max    2023-03-30 00:00:00  -0.617284   7.526882\n",
       "std                    NaN   0.000000   9.505263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 760)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=test_set_feng))\n",
    "display(test_set_feng.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_dataset_ran(config_data: dict) -> pd.DataFrame:\n",
    "    \n",
    "    # Load every set of data\n",
    "    #clean_data = util.pickle_load(config_data['clean_dataset_path'])\n",
    "\n",
    "    x_train_ran = util.pickle_load(config_data[\"train_ran_set_path\"][0])\n",
    "    y_train_ran = util.pickle_load(config_data[\"train_ran_set_path\"][1])\n",
    "\n",
    "    x_valid_ran = util.pickle_load(config_data[\"valid_ran_set_path\"][0])\n",
    "    y_valid_ran = util.pickle_load(config_data[\"valid_ran_set_path\"][1])\n",
    "\n",
    "    x_test_ran = util.pickle_load(config_data[\"test_ran_set_path\"][0])\n",
    "    y_test_ran = util.pickle_load(config_data[\"test_ran_set_path\"][1])\n",
    "\n",
    "    # Concatenate x and y each set\n",
    "    train_set_ran = pd.concat([x_train_ran, y_train_ran], axis = 1)\n",
    "    valid_set_ran = pd.concat([x_valid_ran, y_valid_ran], axis = 1)\n",
    "    test_set_ran = pd.concat([x_test_ran, y_test_ran], axis = 1)\n",
    "\n",
    "    # Return 3 set of data\n",
    "    return train_set_ran, valid_set_ran, test_set_ran"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set_ran, valid_set_ran, test_set_ran = load_dataset_ran(config_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# random cv dataset\n",
    "train_set_ran_feng = transform_to_stock_return(dataset=train_set_ran, params=config_data)\n",
    "train_set_ran_feng = remove_outliers(train_set_ran_feng, 3)\n",
    "\n",
    "val_set_ran_feng = transform_to_stock_return(dataset=valid_set_ran, params=config_data)\n",
    "val_set_ran_feng = remove_outliers(val_set_ran_feng, 3)\n",
    "\n",
    "test_set_ran_feng = transform_to_stock_return(dataset=test_set_ran, params=config_data)\n",
    "test_set_ran_feng = remove_outliers(test_set_ran_feng, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(train_set_ran_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=train_set_ran_feng))\n",
    "display(train_set_ran_feng.shape)\n",
    "\n",
    "display(test_set_ran_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=test_set_ran_feng))\n",
    "display(test_set_ran_feng.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Filter Correlated Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_correlated_features(dataset, params):\n",
    "    #define the target return column name\n",
    "    target_return_column_name = f\"{params['target']} Return D+2\"\n",
    "\n",
    "    # define the correlated features\n",
    "    corr_stock = dataset.corrwith(dataset[target_return_column_name], axis=0).nlargest(10)\n",
    "\n",
    "    # keep correlated features\n",
    "    dataset = dataset[corr_stock]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def feat_selection(dataset):\n",
    "    X = dataset.iloc[:,:-1]\n",
    "    y = dataset.iloc[:,-1]\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # Get feature coefficients from the Lasso model\n",
    "    feature_coefficients = model.coef_\n",
    "\n",
    "    # Create a DataFrame with feature names and their corresponding coefficients\n",
    "    feature_importances = pd.DataFrame({\"feature\": X.columns, \"coefficient\": feature_coefficients})\n",
    "\n",
    "    # Sort the DataFrame by the absolute value of the coefficients in descending order\n",
    "    feature_importances = feature_importances.reindex(feature_importances[\"coefficient\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "    # Get the top 10 features\n",
    "    top_10_features = feature_importances.head(10)[\"feature\"].values\n",
    "\n",
    "    # Print the top 10 features\n",
    "    print(\"Top 10 features:\", top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features: ['LUCY.JK' 'MMLP.JK' 'IBST.JK' 'SLIS.JK' 'FOOD.JK' 'HITS.JK' 'MPPA.JK'\n",
      " 'MPRO.JK' 'PORT.JK' 'YPAS.JK']\n",
      "Top 10 features: ['AIMS.JK' 'BPTR.JK' 'TRUK.JK' 'KONI.JK' 'GZCO.JK' 'PICO.JK' 'HITS.JK'\n",
      " 'PSKT.JK' 'GOLD.JK' 'TGRA.JK']\n",
      "Top 10 features: ['MTSM.JK' 'HDFA.JK' 'FIRE.JK' 'MARI.JK' 'ESTA.JK' 'ALKA.JK' 'MPPA.JK'\n",
      " 'UANG.JK' 'CITY.JK' 'PORT.JK']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.419e-03, tolerance: 7.464e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "feat_selection(train_set_feng)\n",
    "feat_selection(val_set_feng)\n",
    "feat_selection(test_set_feng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_set_feng = transform_to_stock_return(dataset=clean_data, params=config_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "\n",
    "    train_size = int(len(df)*0.7)\n",
    "    val_size = int(len(df)*0.85)\n",
    "\n",
    "    train = df.iloc[:train_size]\n",
    "    validation = df.iloc[train_size:val_size]\n",
    "    test = df.iloc[val_size:]\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_data(clean_set_feng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:,-1:]\n",
    "X_train = train.drop(y_train.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:1568: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['BJBR.JK', 'EXCL.JK', 'SIDO.JK', 'PANI.JK'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on validation set: 3.6844094375838603\n",
      "Mean Squared Error on test set: 2.726104534533604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "target_col_name = y_train.columns\n",
    "# Perform feature selection using Lasso with TimeSeriesSplit cross-validation\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize Lasso with cross-validation\n",
    "alphas = np.logspace(0, 1, 1000)\n",
    "lasso_cv = LassoCV(alphas= alphas, cv=tscv)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[lasso_cv.coef_ != 0]\n",
    "display(selected_features)\n",
    "\n",
    "# 4. Train the final model using the selected features\n",
    "final_model = Lasso(alpha=lasso_cv.alpha_, random_state=42)\n",
    "final_model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# 5. Evaluate the model on the validation set\n",
    "y_validation = val[target_col_name]\n",
    "X_validation = val.drop(target_col_name, axis=1)\n",
    "\n",
    "y_pred_validation = final_model.predict(X_validation[selected_features])\n",
    "mse_validation = mean_squared_error(y_validation, y_pred_validation)\n",
    "print('Mean Squared Error on validation set:', mse_validation)\n",
    "\n",
    "# 6. Evaluate the final model on the test set\n",
    "y_test = test[target_col_name]\n",
    "X_test = test.drop(target_col_name, axis=1)\n",
    "\n",
    "y_pred_test = final_model.predict(X_test[selected_features])\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print('Mean Squared Error on test set:', mse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast for 2023-04-06: -0.19599519499760143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fy/zhbysd194v9gxlrksl8f4bp80000gn/T/ipykernel_3485/2818757078.py:8: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  next_days_data = pd.DataFrame(columns=selected_features, index=pd.date_range(last_day_data.index[-1] + pd.DateOffset(1), periods=num_days, closed='left'))\n"
     ]
    }
   ],
   "source": [
    "# Function to create the features for the next few days\n",
    "def create_features_for_next_days(last_day_data, num_days, selected_features):\n",
    "    # last_day_data: the last row of your original dataset (as a pandas Series or DataFrame)\n",
    "    # num_days: number of days you want to forecast\n",
    "    # selected_features: the list of selected features\n",
    "    \n",
    "    # Create a DataFrame containing the features for the next few days\n",
    "    next_days_data = pd.DataFrame(columns=selected_features, index=pd.date_range(last_day_data.index[-1] + pd.DateOffset(1), periods=num_days, closed='left'))\n",
    "\n",
    "    # Fill in the feature values based on your feature engineering method\n",
    "    # For example, if you have lagged features, you can use the last known values to create the new features\n",
    "\n",
    "    # For this example, let's assume you have lagged features\n",
    "    # We'll use the last known values to create the features for the next few days\n",
    "    for feature in selected_features:\n",
    "        \n",
    "        lag = num_days  # Extract the lag value from the feature name\n",
    "        next_days_data[feature] = last_day_data['BMRI.JK Return D+2'].iloc[-lag:].values\n",
    "\n",
    "    return next_days_data\n",
    "\n",
    "# Create the features for the next few days\n",
    "num_days_to_forecast = 1\n",
    "last_day_data = clean_set_feng.iloc[-1:]\n",
    "next_days_data = create_features_for_next_days(last_day_data, num_days_to_forecast, selected_features)\n",
    "\n",
    "# Predict the target value for the next few days\n",
    "forecast = final_model.predict(next_days_data)\n",
    "\n",
    "# Print the forecast\n",
    "forecast_dates = next_days_data.index\n",
    "for date, value in zip(forecast_dates, forecast):\n",
    "    print(f\"Forecast for {date.strftime('%Y-%m-%d')}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = raw_dataset.iloc[:-2,:]\n",
    "y = raw_dataset[config_data['target']].shift(periods=-2).iloc[:-2]\n",
    "\n",
    "X_train_ran, X_test_ran, y_train_ran, y_test_ran = train_test_split(X, y, test_size = 0.3, random_state = 123)\n",
    "X_val_ran, X_test_ran, y_val_ran, y_test_ran = train_test_split(X_test_ran, y_test_ran, test_size = 0.5, random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(X_train_ran, config_data[\"train_ran_set_path\"][0])\n",
    "util.pickle_dump(y_train_ran, config_data[\"train_ran_set_path\"][1])\n",
    "\n",
    "util.pickle_dump(X_val_ran, config_data[\"valid_ran_set_path\"][0])\n",
    "util.pickle_dump(y_val_ran, config_data[\"valid_ran_set_path\"][1])\n",
    "\n",
    "util.pickle_dump(X_test_ran, config_data[\"test_ran_set_path\"][0])\n",
    "util.pickle_dump(y_test_ran, config_data[\"test_ran_set_path\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
