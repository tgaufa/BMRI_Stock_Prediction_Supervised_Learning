{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import src.util as util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config_data: dict) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "    # Load every set of data\n",
    "    clean_data = util.pickle_load(config_data['clean_dataset_path'])\n",
    "\n",
    "    x_train = util.pickle_load(config_data[\"train_set_path\"][0])\n",
    "    y_train = util.pickle_load(config_data[\"train_set_path\"][1])\n",
    "\n",
    "    x_valid = util.pickle_load(config_data[\"valid_set_path\"][0])\n",
    "    y_valid = util.pickle_load(config_data[\"valid_set_path\"][1])\n",
    "\n",
    "    x_test = util.pickle_load(config_data[\"test_set_path\"][0])\n",
    "    y_test = util.pickle_load(config_data[\"test_set_path\"][1])\n",
    "\n",
    "    # Concatenate x and y each set\n",
    "    train_set = pd.concat([x_train, y_train], axis = 1)\n",
    "    valid_set = pd.concat([x_valid, y_valid], axis = 1)\n",
    "    test_set = pd.concat([x_test, y_test], axis = 1)\n",
    "\n",
    "    # Return 3 set of data\n",
    "    return clean_data, train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data, train_set, valid_set, test_set = load_dataset(config_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stock Return Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the way to normalize all of data value, its relevant if we change them into return percentage.\n",
    "# the advantage are: \n",
    "# 1. the data value will vary from -0.5 to +0.5. While its possible, its less likely stock change will be up/down more than 50% within 2 days. \n",
    "# 2. the stock return is something we want to know anyway therefore its a representative approach in this case\n",
    "\n",
    "def transform_to_stock_return(dataset, params):\n",
    "    # define the return for all stock based on the next day of its price change percentage \n",
    "    dataset = (dataset.shift(periods=1)-dataset)*100/dataset\n",
    "    \n",
    "    #define the target return column name\n",
    "    target_return_column_name = f\"{params['target']} Return D+2\"\n",
    "    \n",
    "    # add additional column of our targeted stock return\n",
    "    dataset[target_return_column_name] = dataset[params['target']].shift(periods=-2)\n",
    "\n",
    "    # handling missing value of shifted targeted column & its reference column\n",
    "    dataset.dropna(subset=params['target'], inplace=True)\n",
    "    dataset.dropna(subset=target_return_column_name, inplace=True)\n",
    "\n",
    "    # handling missing value of the remaining columns\n",
    "    #dataset.fillna(0, inplace=True)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,n_std):\n",
    "    for col in df.columns:\n",
    "        #print('Working on column: {}'.format(col))\n",
    "        \n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean+(n_std*sd))]\n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_feng = transform_to_stock_return(dataset=train_set, params=config_data)\n",
    "train_set_feng = remove_outliers(train_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_feng = transform_to_stock_return(dataset=valid_set, params=config_data)\n",
    "val_set_feng = remove_outliers(val_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_feng = transform_to_stock_return(dataset=test_set, params=config_data)\n",
    "test_set_feng = remove_outliers(test_set_feng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>580</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-08-30 06:44:41.379310336</td>\n",
       "      <td>-0.382604</td>\n",
       "      <td>0.114239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017-01-11 00:00:00</td>\n",
       "      <td>-58.644068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-06-18 18:00:00</td>\n",
       "      <td>-1.612900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019-09-14 12:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020-11-09 06:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.766014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2021-08-30 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.482500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date         Min         Max\n",
       "count                            580  580.000000  580.000000\n",
       "mean   2019-08-30 06:44:41.379310336   -0.382604    0.114239\n",
       "min              2017-01-11 00:00:00  -58.644068    0.000000\n",
       "25%              2018-06-18 18:00:00   -1.612900    0.000000\n",
       "50%              2019-09-14 12:00:00    0.000000    0.000000\n",
       "75%              2020-11-09 06:00:00    0.000000    1.766014\n",
       "max              2021-08-30 00:00:00    0.000000   52.884615\n",
       "std                              NaN    0.000000    5.482500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(580, 104)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=train_set_feng))\n",
    "display(train_set_feng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-01-28 10:11:30.410958848</td>\n",
       "      <td>-0.955964</td>\n",
       "      <td>0.459660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2021-09-10 00:00:00</td>\n",
       "      <td>-25.925926</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-11-17 06:00:00</td>\n",
       "      <td>-3.832011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-01-25 12:00:00</td>\n",
       "      <td>-0.471961</td>\n",
       "      <td>1.452997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-03-31 18:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.263162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022-06-27 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.526882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.528549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date         Min         Max\n",
       "count                            146  146.000000  146.000000\n",
       "mean   2022-01-28 10:11:30.410958848   -0.955964    0.459660\n",
       "min              2021-09-10 00:00:00  -25.925926    0.000000\n",
       "25%              2021-11-17 06:00:00   -3.832011    0.000000\n",
       "50%              2022-01-25 12:00:00   -0.471961    1.452997\n",
       "75%              2022-03-31 18:00:00    0.000000    5.263162\n",
       "max              2022-06-27 00:00:00    0.000000    7.526882\n",
       "std                              NaN    0.000000    6.528549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(146, 104)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(val_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=val_set_feng))\n",
    "display(val_set_feng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>134</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-11-18 23:06:16.119403008</td>\n",
       "      <td>-0.732008</td>\n",
       "      <td>0.408796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2022-07-05 00:00:00</td>\n",
       "      <td>-25.882353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2022-09-20 06:00:00</td>\n",
       "      <td>-3.013468</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-11-22 00:00:00</td>\n",
       "      <td>-0.466203</td>\n",
       "      <td>0.893992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-01-24 18:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.182188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-04-05 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.526882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.333179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date         Min         Max\n",
       "count                            134  134.000000  134.000000\n",
       "mean   2022-11-18 23:06:16.119403008   -0.732008    0.408796\n",
       "min              2022-07-05 00:00:00  -25.882353    0.000000\n",
       "25%              2022-09-20 06:00:00   -3.013468    0.000000\n",
       "50%              2022-11-22 00:00:00   -0.466203    0.893992\n",
       "75%              2023-01-24 18:00:00    0.000000    4.182188\n",
       "max              2023-04-05 00:00:00    0.000000    7.526882\n",
       "std                              NaN    0.000000    6.333179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(134, 104)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_set_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=test_set_feng))\n",
    "display(test_set_feng.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_dataset_ran(config_data: dict) -> pd.DataFrame:\n",
    "    \n",
    "    # Load every set of data\n",
    "    #clean_data = util.pickle_load(config_data['clean_dataset_path'])\n",
    "\n",
    "    x_train_ran = util.pickle_load(config_data[\"train_ran_set_path\"][0])\n",
    "    y_train_ran = util.pickle_load(config_data[\"train_ran_set_path\"][1])\n",
    "\n",
    "    x_valid_ran = util.pickle_load(config_data[\"valid_ran_set_path\"][0])\n",
    "    y_valid_ran = util.pickle_load(config_data[\"valid_ran_set_path\"][1])\n",
    "\n",
    "    x_test_ran = util.pickle_load(config_data[\"test_ran_set_path\"][0])\n",
    "    y_test_ran = util.pickle_load(config_data[\"test_ran_set_path\"][1])\n",
    "\n",
    "    # Concatenate x and y each set\n",
    "    train_set_ran = pd.concat([x_train_ran, y_train_ran], axis = 1)\n",
    "    valid_set_ran = pd.concat([x_valid_ran, y_valid_ran], axis = 1)\n",
    "    test_set_ran = pd.concat([x_test_ran, y_test_ran], axis = 1)\n",
    "\n",
    "    # Return 3 set of data\n",
    "    return train_set_ran, valid_set_ran, test_set_ran"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set_ran, valid_set_ran, test_set_ran = load_dataset_ran(config_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# random cv dataset\n",
    "train_set_ran_feng = transform_to_stock_return(dataset=train_set_ran, params=config_data)\n",
    "train_set_ran_feng = remove_outliers(train_set_ran_feng, 3)\n",
    "\n",
    "val_set_ran_feng = transform_to_stock_return(dataset=valid_set_ran, params=config_data)\n",
    "val_set_ran_feng = remove_outliers(val_set_ran_feng, 3)\n",
    "\n",
    "test_set_ran_feng = transform_to_stock_return(dataset=test_set_ran, params=config_data)\n",
    "test_set_ran_feng = remove_outliers(test_set_ran_feng, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(train_set_ran_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=train_set_ran_feng))\n",
    "display(train_set_ran_feng.shape)\n",
    "\n",
    "display(test_set_ran_feng.isna().any().sum())\n",
    "display(util.summary_dataset_describe(dataset=test_set_ran_feng))\n",
    "display(test_set_ran_feng.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Filter Correlated Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_correlated_features(dataset, params):\n",
    "    #define the target return column name\n",
    "    target_return_column_name = f\"{params['target']} Return D+2\"\n",
    "\n",
    "    # define the correlated features\n",
    "    corr_stock = dataset.corrwith(dataset[target_return_column_name], axis=0).nlargest(10)\n",
    "\n",
    "    # keep correlated features\n",
    "    dataset = dataset[corr_stock]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def feat_selection(dataset):\n",
    "    X = dataset.iloc[:,:-1]\n",
    "    y = dataset.iloc[:,-1]\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X,y)\n",
    "\n",
    "    # Get feature coefficients from the Lasso model\n",
    "    feature_coefficients = model.coef_\n",
    "\n",
    "    # Create a DataFrame with feature names and their corresponding coefficients\n",
    "    feature_importances = pd.DataFrame({\"feature\": X.columns, \"coefficient\": feature_coefficients})\n",
    "\n",
    "    # Sort the DataFrame by the absolute value of the coefficients in descending order\n",
    "    feature_importances = feature_importances.reindex(feature_importances[\"coefficient\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "    # Get the top 10 features\n",
    "    top_10_features = feature_importances.head(10)[\"feature\"].values\n",
    "\n",
    "    # Print the top 10 features\n",
    "    print(\"Top 10 features:\", top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features: ['PTSP.JK' 'AALI.JK' 'BCIC.JK' 'TGKA.JK' 'FAST.JK' 'DUTI.JK' 'BHIT.JK'\n",
      " 'MEDC.JK' 'LPCK.JK' 'KLBF.JK']\n",
      "Top 10 features: ['BBCA.JK' 'ASBI.JK' 'BEKS.JK' 'KBLI.JK' 'EMTK.JK' 'BNBR.JK' 'PNLF.JK'\n",
      " 'JPFA.JK' 'JKON.JK' 'TBIG.JK']\n",
      "Top 10 features: ['DPNS.JK' 'PTSN.JK' 'PALM.JK' 'HRUM.JK' 'KLBF.JK' 'SMCB.JK' 'BABP.JK'\n",
      " 'MEGA.JK' 'ERAA.JK' 'IBST.JK']\n"
     ]
    }
   ],
   "source": [
    "feat_selection(train_set_feng)\n",
    "feat_selection(val_set_feng)\n",
    "feat_selection(test_set_feng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_set_feng = transform_to_stock_return(dataset=clean_data, params=config_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "\n",
    "    train_size = int(len(df)*0.7)\n",
    "    val_size = int(len(df)*0.85)\n",
    "\n",
    "    train = df.iloc[:train_size]\n",
    "    validation = df.iloc[train_size:val_size]\n",
    "    test = df.iloc[val_size:]\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split_data(clean_set_feng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:,-1:]\n",
    "X_train = train.drop(y_train.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:1568: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['BCIC.JK'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on validation set: 2.591793762955886\n",
      "Mean Squared Error on test set: 3.4585836991195076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "target_col_name = y_train.columns\n",
    "# Perform feature selection using Lasso with TimeSeriesSplit cross-validation\n",
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize Lasso with cross-validation\n",
    "alphas = np.logspace(0, 1, 1000)\n",
    "lasso_cv = LassoCV(alphas= alphas, cv=tscv)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[lasso_cv.coef_ != 0]\n",
    "display(selected_features)\n",
    "\n",
    "# 4. Train the final model using the selected features\n",
    "final_model = Lasso(alpha=lasso_cv.alpha_, random_state=42)\n",
    "final_model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# 5. Evaluate the model on the validation set\n",
    "y_validation = val[target_col_name]\n",
    "X_validation = val.drop(target_col_name, axis=1)\n",
    "\n",
    "y_pred_validation = final_model.predict(X_validation[selected_features])\n",
    "mse_validation = mean_squared_error(y_validation, y_pred_validation)\n",
    "print('Mean Squared Error on validation set:', mse_validation)\n",
    "\n",
    "# 6. Evaluate the final model on the test set\n",
    "y_test = test[target_col_name]\n",
    "X_test = test.drop(target_col_name, axis=1)\n",
    "\n",
    "y_pred_test = final_model.predict(X_test[selected_features])\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print('Mean Squared Error on test set:', mse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fy/zhbysd194v9gxlrksl8f4bp80000gn/T/ipykernel_3191/411603654.py:8: FutureWarning: Argument `closed` is deprecated in favor of `inclusive`.\n",
      "  next_days_data = pd.DataFrame(columns=selected_features, index=pd.date_range(last_day_data.index[-1] + pd.DateOffset(1), periods=num_days, closed='left'))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m next_days_data \u001b[39m=\u001b[39m create_features_for_next_days(last_day_data, num_days_to_forecast, selected_features)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Predict the target value for the next few days\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m forecast \u001b[39m=\u001b[39m final_model\u001b[39m.\u001b[39;49mpredict(next_days_data)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Print the forecast\u001b[39;00m\n\u001b[1;32m     31\u001b[0m forecast_dates \u001b[39m=\u001b[39m next_days_data\u001b[39m.\u001b[39mindex\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_base.py:354\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    341\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:1074\u001b[0m, in \u001b[0;36mElasticNet._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_base.py:337\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    335\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Function to create the features for the next few days\n",
    "def create_features_for_next_days(last_day_data, num_days, selected_features):\n",
    "    # last_day_data: the last row of your original dataset (as a pandas Series or DataFrame)\n",
    "    # num_days: number of days you want to forecast\n",
    "    # selected_features: the list of selected features\n",
    "    \n",
    "    # Create a DataFrame containing the features for the next few days\n",
    "    next_days_data = pd.DataFrame(columns=selected_features, index=pd.date_range(last_day_data.index[-1] + pd.DateOffset(1), periods=num_days, closed='left'))\n",
    "\n",
    "    # Fill in the feature values based on your feature engineering method\n",
    "    # For example, if you have lagged features, you can use the last known values to create the new features\n",
    "\n",
    "    # For this example, let's assume you have lagged features\n",
    "    # We'll use the last known values to create the features for the next few days\n",
    "    for feature in selected_features:\n",
    "        if 'lag' in feature:\n",
    "            lag = int(feature.split('_')[-1])  # Extract the lag value from the feature name\n",
    "            next_days_data[feature] = last_day_data['value'].iloc[-lag:].values\n",
    "\n",
    "    return next_days_data\n",
    "\n",
    "# Create the features for the next few days\n",
    "num_days_to_forecast = 5\n",
    "last_day_data = clean_set_feng.iloc[-1:]\n",
    "next_days_data = create_features_for_next_days(last_day_data, num_days_to_forecast, selected_features)\n",
    "\n",
    "# Predict the target value for the next few days\n",
    "forecast = final_model.predict(next_days_data)\n",
    "\n",
    "# Print the forecast\n",
    "forecast_dates = next_days_data.index\n",
    "for date, value in zip(forecast_dates, forecast):\n",
    "    print(f\"Forecast for {date.strftime('%Y-%m-%d')}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
